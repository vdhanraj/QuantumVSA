# === Paths ===
curr_dir: "~/Neurosymbolic-LLM/Programs"
git_dir: "~/Neurosymbolic-LLM"
ckpt_dir: "~/.llama/checkpoints/Llama3.1-8B-Instruct"
tokenizer_path: "~/.llama/checkpoints/Llama3.1-8B-Instruct/tokenizer.model"
generate_data: 1
log_wandb: 1
gpu_seed: 0

# === Model Config ===
max_seq_len: 10000
max_batch_size: 2
model_parallel_size: 1
model_dim: 4096

# === Sampling Parameters ===
top_p: 0.9
temperature: 0.0
max_gen_len: null  # or use null for Python None

# === Symbolic Engine ===
max_digits: 5
VSA_dim: 2048
possible_problems:
  - addition
  - multiplication
  - division
  - modulo
  - gcd
  - lcm
  - square_mod
  - bitwise_and
  - bitwise_xor
  - bitwise_or

# === Data Collection ===
train_data_rounds: 10000
val_data_rounds: 100
test_data_rounds: 1000
restrict_train_dataset: 0
restrict_val_dataset: 0
restrict_test_dataset: 0
save_frequency: 50
layer_numbers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,
                16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]
complexity: 2
n_samples: 2
problem_type:
  - multiplication
  - modulo
  - gcd
  - lcm
  - square_mod
  - bitwise_and
  - bitwise_xor
  - bitwise_or

# === Tokenization ===
tokens_to_keep: 1
calculate_end_index: 0

# === Encoder/Decoder Training ===
encoder_decoder_batch_size: 512
training_epochs: 1000
learning_rate: 0.001
learning_rate_reduction_factors:
  50: 0.5
  100: 0.5
  250: 0.1
  500: 0.4
train_freq_print: 100

decoding_epochs: 1000
decoding_learning_rate: 0.001
decoding_learning_rate_reduction_factors:
  10: 0.1
  25: 0.5
  100: 0.5
  250: 0.4

training_data_df_path: ''
val_data_df_path: ''
testing_data_df_path: ''